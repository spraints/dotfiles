#!/usr/bin/env ruby
#/ Usage: prs [--count N] [--last [N] day|week|month|year]
#/ Show my PRs.

require "time"

require_relative "github-api"

GRAY = 30
RED = 31
YELLOW = 33
GREEN = 32

def main(token:, count: 20, earliest: (Time.now - (90 * DAY)), ignore_trains: false, cached: false)
  earliest = earliest.to_datetime
  results =
    if cached
      get_cached_results
    else
      get_data(token: token, query: QUERY, variables: {count: count}, features: [:merge_queue])
    end
  if errors = results["errors"]
    p errors
    exit 1
  end
  if !cached
    write_cache(results)
  end
  prs =
    results.dig("data", "assigned", "nodes") +
    results.dig("data", "viewer", "pullRequests", "nodes")
  viewed = {}
  prs.each do |pr|
    permalink = pr.fetch("permalink")
    if viewed[permalink]
      #puts "skip duplicate #{permalink}"
      next
    end
    viewed[permalink] = true

    updated_at = DateTime.parse(pr.fetch("updatedAt"))
    archived = pr.fetch("repository").fetch("isArchived")
    if updated_at < earliest || archived
      #puts "skip obsolete #{permalink} #{updated_at} #{earliest} #{archived.inspect}"
      next
    end

    case pr.fetch("__typename")
    when "PullRequest"
      show_pr(pr, ignore_trains: ignore_trains)
    when "Issue"
      show_issue(pr)
    else
      puts "SKIP #{pr.fetch("__typename").inspect} (#{pr.keys.sort.inspect})"
    end
  end
end

def get_cached_results
  JSON.load(File.read(cache_file))
end

def write_cache(data)
  File.write(cache_file, JSON.dump(data))
end

def cache_file
  File.join(ENV["HOME"], ".prs-cache")
end

def show_issue(pr)
    permalink = pr.fetch("permalink")
    is_read = pr.fetch("isReadByViewer")
    author = pr.fetch("author").fetch("login")
    title = pr.fetch("title")
    updated_at = DateTime.parse(pr.fetch("updatedAt"))
    ago = time_in_words(updated_at)

    color, dot, status = 30, "â–ª", "ISSUE"

    read_dot =
      if is_read
        " "
      else
        mail = "âœ‰"
        "\033[30m#{mail}\033[0m"
      end

    printf("%<read_dot>s \033[%<color>dm%<dot>s %<status>-15s\033[0m %<permalink>s [by %<author>s] %<title>s, updated %<ago>s\n", {
      read_dot: read_dot,
      color: color,
      dot: dot,
      status: status,
      permalink: permalink,
      author: author,
      title: title,
      ago: ago,
    })
end

def show_pr(pr, ignore_trains:)
    permalink = pr.fetch("permalink")
    updated_at = DateTime.parse(pr.fetch("updatedAt"))
    author = pr.fetch("author").fetch("login")
    title = pr.fetch("title")
    draft = pr.fetch("isDraft")
    is_read = pr.fetch("isReadByViewer")
    decision = pr.fetch("reviewDecision")
    ago = time_in_words(updated_at)
    check_status = pr.dig("headRef", "target", "statusCheckRollup", "state")
    branch = pr.fetch("headRefName")
    mergeable = pr.fetch("mergeable")

    status_parts = []

    status_parts <<
      if draft
        [GRAY, "DRAFT  "]
      else
        case decision
        when "REQ_CHG"
          [RED, decision]
        when "APPROVED"
          [GREEN, "APPROVE"]
        when "REVIEW_REQUIRED"
          [YELLOW, "REV_REQ"]
        else
          [YELLOW, decision]
        end
      end

    status_parts <<
      case check_status
      when NilClass
        [GRAY, "  "]
      when "SUCCESS"
        [GREEN, "CI"]
      when "PENDING"
        [YELLOW, "CI"]
      when "FAILURE"
        [RED, "CI"]
      else
        [RED, "CI_#{check_status}"]
      end

    status_parts <<
      case mergeable
      when "MERGEABLE"
        [GREEN, "MERGE_OK"]
      when "UNKNOWN"
        [YELLOW, "check_mg"]
      else
        [RED, mergeable]
      end

    colors = status_parts.group_by { |color, _| color }

    status_parts.unshift \
      case
      when colors[GRAY]
        [GRAY, "â–ª"]
      when colors[RED]
        [RED, "âœ–ï¸Ž"]
      when colors[YELLOW]
        [YELLOW, "â—"]
      when colors[GREEN]
        [GREEN, "âœ”ï¸Ž"]
      else
        [GRAY, "?"]
      end

    read_dot =
      if is_read
        " "
      else
        mail = "âœ‰"
        color(mail, GRAY)
      end

    printf("%<read_dot>s %<statuses>s %<permalink>s [%<branch>s by %<author>s] %<title>s, updated %<ago>s\n", {
      read_dot: read_dot,
      statuses: status_parts.map { |c, text| color(text, c) }.join(" "),
      permalink: permalink,
      branch: branch,
      author: author,
      title: title,
      ago: ago,
    })
    show_train_info(pr) unless ignore_trains
    show_mq_info(pr)
end

def show_train_info(pr)
  if timeline_items = pr.dig("timelineItems", "nodes")
    timeline_items.each do |item|
      source = item.fetch("source")
      next if source.nil? || source.empty?
      state = source.fetch("state")
      branch = source.fetch("headRefName")
      url = source.fetch("url")
      source_labels = source.dig("labels", "nodes")&.map { |label| label.fetch("name") }
      train_label = source_labels.grep(/Deploy train/).first

      if state == "OPEN" && train_label
        puts "    [#{branch}] #{url} #{source_labels.map { |label| "(#{label})" }.join(" ")}"
      end
    end
  end
end

def show_mq_info(pr)
  if mq = pr.dig("mergeQueueEntry")
    state = mq.fetch("state")
    position = mq.fetch("position")
    ttm = human_duration(mq.fetch("estimatedTimeToMerge"))
    solo_str = mq.fetch("solo") ? " SOLO" : ""
    head_url = mq.dig("headCommit", "url")
    ci_state = mq.dig("headCommit", "statusCheckRollup", "state")
    group_size = mq.dig("mergeQueue", "configuration", "maximumEntriesToMerge").to_i
    mq_url = mq.dig("mergeQueue", "url")

    state_color =
      case state
      when "QUEUED"
        GRAY
      when "AWAITING_CHECKS"
        YELLOW
      when "MERGEABLE"
        GREEN
      else
        RED
      end

    ci_state_color =
      case ci_state
      when "EXPECTED"
        GRAY
      when "PENDING"
        YELLOW
      when "SUCCESS"
        GREEN
      else
        RED
      end

    printf "   ðŸ¤– %<state>s spot=%<position>2d group=%<group_size>2d%<solo_str>s %<mq_url>s\n      merging in %<ttm>s - %<head_url>s (%<ci_state>s)\n",
      ci_state: color(ci_state, ci_state_color),
      group_size: group_size,
      head_url: head_url,
      position: position,
      solo_str: solo_str,
      state: color(state, state_color),
      ttm: ttm,
      mq_url: mq_url
  end
end

DAY = 60 * 60 * 24

def time_in_words(dt)
  ago = (Time.now - dt.to_time).to_i
  days = ago / DAY
  months = days / 30
  if days < 1
    "in the last day"
  elsif days < 7
    "in the last #{days + 1} days"
  elsif months < 1
    "in the last month"
  else
    "in the last #{months + 1} months"
  end
end

def human_duration(seconds, prefix = "")
  return "(unknown)" unless seconds
  if seconds > 3600
    h = seconds / 3600
    s = seconds % 3600
    "#{prefix}#{h}h#{human_duration(s, " ")}"
  elsif seconds > 60
    m = seconds / 60
    s = seconds % 60
    "#{prefix}#{m}m#{human_duration(s, " ")}"
  elsif seconds > 0
    "#{prefix}#{seconds}s"
  else
    ""
  end
end

def color(text, color_code)
  "\033[#{color_code}m#{text}\033[0m"
end

def usage
  puts File.read(__FILE__).lines.grep(/^#\//).join.gsub(/^#\/ /, '')
  exit 1
end

QUERY = <<QUERY
query ($count: Int) {
  assigned: search(
    type: ISSUE
    query: "assignee:spraints state:open"
    first: $count
  ) {
    nodes {
      __typename
      ...pullRequestDetails
      ...issueDetails
    }
  }
  viewer {
    pullRequests(
      states: [OPEN]
      first: $count
      orderBy: { field: CREATED_AT, direction: DESC }
    ) {
      nodes {
        __typename
        ...pullRequestDetails
      }
    }
  }
}
fragment issueDetails on Issue {
  id
  ...rnd
  ...comm
  ...issonly
}
fragment pullRequestDetails on PullRequest {
  id
  ...rnd
  ...comm
  ...pronly
}
fragment rnd on RepositoryNode {
  repository {
    isArchived
  }
}
fragment comm on Comment {
  author {
    login
  }
  createdAt
  updatedAt
}
fragment issonly on Issue {
  title
  permalink: url
  isReadByViewer
}
fragment pronly on PullRequest {
  title
  permalink
  headRefName
  mergeable
  isDraft
  isReadByViewer
  reviewDecision
  mergeQueueEntry {
    estimatedTimeToMerge
    solo
    position
    state
    headCommit {
      url
      statusCheckRollup {
        state
      }
    }
    mergeQueue {
      configuration {
        maximumEntriesToMerge
      }
      url
    }
  }


  headRef {
    target {
      ... on Commit {
        statusCheckRollup {
          state
        }
      }
    }
  }
  timelineItems(last: 10, itemTypes: [CROSS_REFERENCED_EVENT]) {
    nodes {
      ... on CrossReferencedEvent {
        source {
          ... on PullRequest {
            labels(first: 10) {
              nodes {
                name
              }
            }
            headRefName
            state
            url
          }
        }
      }
    }
  }
}
QUERY

opts = {}

WEEK = 7 * DAY
MONTH = 32 * DAY
YEAR = 366 * DAY
INT_RE = /\A\d+\z/

def parse_since_args
  case ARGV.first
  when INT_RE
    n = ARGV.shift.to_i
    n * parse_time_unit_arg
  else
    parse_time_unit_arg
  end
end

def parse_time_unit_arg
  case arg = ARGV.shift
  when /\Aday/
    DAY
  when /\Aweek/
    WEEK
  when /\Amonth/
    MONTH
  when /\Ayear/
    YEAR
  else
    $stderr.puts "Unrecognized time value #{arg.inspect}"
    exit 1
  end
end

while ARGV.first.to_s =~ /^-/
  case ARGV.shift
  when "--count"
    opts[:count] = ARGV.shift.to_i
  when "--last"
    opts[:earliest] = Time.now - parse_since_args
  when "--query"
    puts QUERY
    exit 0
  when /^--cach/
    opts[:cached] = true
  else
    usage
  end
end

usage if ARGV.size > 0

unless opts[:token] = ENV["GITHUB_TOKEN"]
  token_file = File.read(File.join(ENV["HOME"], ".github-token"))
  if token_file =~ /(^| )GITHUB_TOKEN=(\w+)/
    opts[:token] = $2
  end
end

main(opts)
