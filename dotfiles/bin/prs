#!/usr/bin/env ruby
#/ Usage: prs [--count N] [--last [N] day|week|month|year]
#/ Show my PRs.

require "time"

require_relative "github-api"

def main(token:, count: 20, earliest: (Time.now - (90 * DAY)), ignore_trains: false, cached: false)
  earliest = earliest.to_datetime
  results =
    if cached
      get_cached_results
    else
      get_data(token: token, query: QUERY, variables: {count: count}, features: [:merge_queue])
    end
  if errors = results["errors"]
    p errors
    exit 1
  end
  if !cached
    write_cache(results)
  end
  prs =
    results.dig("data", "assigned", "nodes") +
    results.dig("data", "viewer", "pullRequests", "nodes")
  viewed = {}
  prs.each do |pr|
    permalink = pr.fetch("permalink")
    if viewed[permalink]
      #puts "skip duplicate #{permalink}"
      next
    end
    viewed[permalink] = true

    updated_at = DateTime.parse(pr.fetch("updatedAt"))
    archived = pr.fetch("repository").fetch("isArchived")
    if updated_at < earliest || archived
      #puts "skip obsolete #{permalink} #{updated_at} #{earliest} #{archived.inspect}"
      next
    end

    case pr.fetch("__typename")
    when "PullRequest"
      show_pr(pr, ignore_trains: ignore_trains)
    when "Issue"
      show_issue(pr)
    else
      puts "SKIP #{pr.fetch("__typename").inspect} (#{pr.keys.sort.inspect})"
    end
  end
end

def get_cached_results
  JSON.load(File.read(cache_file))
end

def write_cache(data)
  File.write(cache_file, JSON.dump(data))
end

def cache_file
  File.join(ENV["HOME"], ".prs-cache")
end

def show_issue(pr)
    permalink = pr.fetch("permalink")
    is_read = pr.fetch("isReadByViewer")
    author = pr.fetch("author").fetch("login")
    title = pr.fetch("title")
    updated_at = DateTime.parse(pr.fetch("updatedAt"))
    ago = time_in_words(updated_at)

    color, dot, status = 30, "‚ñ™", "ISSUE"

    read_dot =
      if is_read
        " "
      else
        mail = "‚úâ"
        "\033[30m#{mail}\033[0m"
      end

    printf("%<read_dot>s \033[%<color>dm%<dot>s %<status>-15s\033[0m %<permalink>s [by %<author>s] %<title>s, updated %<ago>s\n", {
      read_dot: read_dot,
      color: color,
      dot: dot,
      status: status,
      permalink: permalink,
      author: author,
      title: title,
      ago: ago,
    })
end

def show_pr(pr, ignore_trains:)
    permalink = pr.fetch("permalink")
    updated_at = DateTime.parse(pr.fetch("updatedAt"))
    author = pr.fetch("author").fetch("login")
    title = pr.fetch("title")
    draft = pr.fetch("isDraft")
    is_read = pr.fetch("isReadByViewer")
    decision = pr.fetch("reviewDecision")
    ago = time_in_words(updated_at)
    check_status = pr.dig("headRef", "target", "statusCheckRollup", "state")
    branch = pr.fetch("headRefName")
    mergeable = pr.fetch("mergeable")

    color, dot, status =
      if draft
        [30, "‚ñ™", "DRAFT"]
      elsif check_status != "SUCCESS" && check_status != "PENDING" && !check_status.nil?
        [31, "‚úñÔ∏é", "CI_#{check_status}"]
      elsif mergeable != "MERGEABLE" && mergeable != "UNKNOWN"
        [31, "‚úñÔ∏é", mergeable]
      elsif decision == "CHANGES_REQUESTED"
        [31, "‚úñÔ∏é", decision]
      elsif check_status == "PENDING"
        [33, "‚óè", "CI_#{check_status}"]
      elsif decision != "APPROVED" && !decision.nil?
        [33, "‚óè", decision]
      elsif mergeable == "UNKNOWN"
        [32, "‚úîÔ∏é", "checking_merge"]
      else
        [32, "‚úîÔ∏é", decision || "no_review"]
      end

    status_parts = [
      check_status && "CI_#{check_status}",
      mergeable,
      decision,
    ].compact.join(",")

    read_dot =
      if is_read
        " "
      else
        mail = "‚úâ"
        "\033[30m#{mail}\033[0m"
      end

    printf("%<read_dot>s \033[%<color>dm%<dot>s %<status>-15s\033[0m %<permalink>s [%<branch>s by %<author>s] %<title>s, updated %<ago>s\n", {
      read_dot: read_dot,
      color: color,
      dot: dot,
      status: status,
      permalink: permalink,
      branch: branch,
      author: author,
      title: title,
      ago: ago,
    })
    show_train_info(pr) unless ignore_trains
    show_mq_info(pr)
end

def show_train_info(pr)
  if timeline_items = pr.dig("timelineItems", "nodes")
    timeline_items.each do |item|
      source = item.fetch("source")
      next if source.nil? || source.empty?
      state = source.fetch("state")
      branch = source.fetch("headRefName")
      url = source.fetch("url")
      source_labels = source.dig("labels", "nodes")&.map { |label| label.fetch("name") }
      train_label = source_labels.grep(/Deploy train/).first

      if state == "OPEN" && train_label
        puts "    [#{branch}] #{url} #{source_labels.map { |label| "(#{label})" }.join(" ")}"
      end
    end
  end
end

def show_mq_info(pr)
  if mq = pr.dig("mergeQueueEntry")
    state = mq.fetch("state")
    position = mq.fetch("position")
    ttm = human_duration(mq.fetch("estimatedTimeToMerge"))
    solo_str = mq.fetch("solo") ? " SOLO" : ""
    head_url = mq.dig("headCommit", "url")
    ci_state = mq.dig("headCommit", "statusCheckRollup", "state")
    # Align the commit URL with the PR url on the line above
    printf "     ü§ñ queued      %<head_url>s %<solo_str>s spot=%<position>d merging in %<ttm>s - mq:%<state>s/ci:%<ci_state>s\n",
      state: state,
      ci_state: ci_state,
      position: position,
      ttm: ttm,
      solo_str: solo_str,
      head_url: head_url
  end
end

DAY = 60 * 60 * 24

def time_in_words(dt)
  ago = (Time.now - dt.to_time).to_i
  days = ago / DAY
  months = days / 30
  if days < 1
    "in the last day"
  elsif days < 7
    "in the last #{days + 1} days"
  elsif months < 1
    "in the last month"
  else
    "in the last #{months + 1} months"
  end
end

def human_duration(seconds)
  "#{seconds} seconds" # TODO
end

def usage
  puts File.read(__FILE__).lines.grep(/^#\//).join.gsub(/^#\/ /, '')
  exit 1
end

QUERY = <<QUERY
query ($count: Int) {
  assigned: search(
    type: ISSUE
    query: "assignee:spraints state:open"
    first: $count
  ) {
    nodes {
      __typename
      ...pullRequestDetails
      ...issueDetails
    }
  }
  viewer {
    pullRequests(
      states: [OPEN]
      first: $count
      orderBy: { field: CREATED_AT, direction: DESC }
    ) {
      nodes {
        __typename
        ...pullRequestDetails
      }
    }
  }
}
fragment issueDetails on Issue {
  ...rnd
  ...comm
  ...issonly
}
fragment pullRequestDetails on PullRequest {
  ...rnd
  ...comm
  ...pronly
}
fragment rnd on RepositoryNode {
  repository {
    isArchived
  }
}
fragment comm on Comment {
  author {
    login
  }
  createdAt
  updatedAt
}
fragment issonly on Issue {
  title
  permalink: url
  isReadByViewer
}
fragment pronly on PullRequest {
  title
  permalink
  headRefName
  mergeable
  isDraft
  isReadByViewer
  reviewDecision
  mergeQueueEntry {
    estimatedTimeToMerge
    solo
    position
    state
    headCommit {
      url
      statusCheckRollup {
        state
      }
    }
  }


  headRef {
    target {
      ... on Commit {
        statusCheckRollup {
          state
        }
      }
    }
  }
  timelineItems(last: 10, itemTypes: [CROSS_REFERENCED_EVENT]) {
    nodes {
      ... on CrossReferencedEvent {
        source {
          ... on PullRequest {
            labels(first: 10) {
              nodes {
                name
              }
            }
            headRefName
            state
            url
          }
        }
      }
    }
  }
}
QUERY

opts = {}

WEEK = 7 * DAY
MONTH = 32 * DAY
YEAR = 366 * DAY
INT_RE = /\A\d+\z/

def parse_since_args
  case ARGV.first
  when INT_RE
    n = ARGV.shift.to_i
    n * parse_time_unit_arg
  else
    parse_time_unit_arg
  end
end

def parse_time_unit_arg
  case arg = ARGV.shift
  when /\Aday/
    DAY
  when /\Aweek/
    WEEK
  when /\Amonth/
    MONTH
  when /\Ayear/
    YEAR
  else
    $stderr.puts "Unrecognized time value #{arg.inspect}"
    exit 1
  end
end

while ARGV.first.to_s =~ /^-/
  case ARGV.shift
  when "--count"
    opts[:count] = ARGV.shift.to_i
  when "--last"
    opts[:earliest] = Time.now - parse_since_args
  when "--query"
    puts QUERY
    exit 0
  when /^--cach/
    opts[:cached] = true
  else
    usage
  end
end

usage if ARGV.size > 0

unless opts[:token] = ENV["GITHUB_TOKEN"]
  token_file = File.read(File.join(ENV["HOME"], ".github-token"))
  if token_file =~ /(^| )GITHUB_TOKEN=(\w+)/
    opts[:token] = $2
  end
end

main(opts)
